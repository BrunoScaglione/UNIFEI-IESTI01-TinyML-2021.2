{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of TF_Boston_Housing_Regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo2JiEtkzvQL"
      },
      "source": [
        "# House Pricing Regression using Dense Neural Network (DNN) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z56YN3JZ0Duh"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfPnR74Gzc_q"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrGLDV6q0PCc"
      },
      "source": [
        "## Upload and Explore Dataset\n",
        "[The Boston house-price data](http://lib.stat.cmu.edu/datasets/boston) \n",
        "* This is a dataset taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
        "* There are 506 samples, each one with 13 attributes (Features `Xi` from 0 to 12) of houses at different locations around the Boston suburbs in the late 1970s. The attributes themselves are defined in the StatLib website (as per capta crime rate in the area, number of rooms, distance  from employemment center, etc).\n",
        "- Target (`Y`) is the median values of the houses at a location (in USD 1,000).\n",
        "\n",
        "**Goal**\n",
        "*  Our goal is to build a regression model that takes these **13 features as input** and **output a single value prediction** of the \"median value of owner-occupied homes (in USD 1000).\"\n",
        "* Dataset can be download direct from: [tf.keras.datasets.boston_housing](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/boston_housing/load_data)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib2Ihepx0N6h"
      },
      "source": [
        "data = tf.keras.datasets.boston_housing\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = data.load_data()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pq1bMIRd38b8"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHaUYnSs4bq4"
      },
      "source": [
        "print(y_test.shape)\n",
        "print(y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ENgGg2Qudp"
      },
      "source": [
        "### Exploring Target (Y)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imESCCOODh8K"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82O77YHNybhA"
      },
      "source": [
        "print('Min price in $K:  ',y_train.min())\n",
        "print('Max price in $K:  ',round(y_train.mean(),2))\n",
        "print('Mean price in $K: ',y_train.max())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SCj8H9rysOM"
      },
      "source": [
        "plt.hist(y_train, label='train')\n",
        "plt.hist(y_test, label = 'test')\n",
        "plt.xlabel('Price in K$')\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyfWBrrW9gAd"
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMu0y_Az4hRr"
      },
      "source": [
        "x_train[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_M47lzg7z3vI"
      },
      "source": [
        "### Exploring Input Features (X)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgy3E-Y7D__F"
      },
      "source": [
        "for i in range(len(x_train[0])):\n",
        "  print(\"Feature {} ==> range from {} to {}\".format(\n",
        "      i, x_train[:,i].min(), x_train[:,i].max()\n",
        "      )\n",
        "  )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXAskOWeGHSx"
      },
      "source": [
        "feature = 1\n",
        "plt.hist(x_train[:,feature], label='train')\n",
        "plt.hist(x_test[:,feature], label = 'test')\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btqWSp9dSrCB"
      },
      "source": [
        "print (x_train.max())\n",
        "print (x_train.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-H0LwB_7von"
      },
      "source": [
        "### Preprocessing Data \n",
        "\n",
        "**Normalizing Data**: \n",
        "We notice that values range varies depending on the type of the feature. If we are training a neural network, for various reasons it's easier if we treat all values as between 0 and 1 (or at least with similar ranges), a process called 'normalizing'. In this case, all features will be `rescaled`.\n",
        "\n",
        "The standard score of a sample `x` is calculated as:\n",
        "\n",
        "        z = (x - u) / s\n",
        "\n",
        "where `u` is the mean of the training samples or zero if `with_mean=False` and `s` is the standard deviation of the training samples or one if `with_std=False`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZrGVMsH0X3F"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# first we fit the scaler on the training dataset\n",
        "scaler.fit(x_train)\n",
        "\n",
        "# then we call the transform method to scale both the training and testing data\n",
        "x_train_norm = scaler.transform(x_train)\n",
        "x_test_norm = scaler.transform(x_test)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG_fd0Ig5BuM"
      },
      "source": [
        "Another way do normalize data directly with numpy is:\n",
        "- Get per-feature statistics (mean, standard deviation) from the training set to normalize by:\n",
        "  - x_train_mean = np.mean(x_train, axis=0)\n",
        "  - x_train_std = np.std(x_train, axis=0)\n",
        "  - x_train_norm = (x_train - x_train_mean) / x_train_std\n",
        "\n",
        "  - x_test_norm = (x_test - x_train_mean) / x_train_std\n",
        "\n",
        "**Note** that the quantities used for normalizing the test data are computed using the training data. You should never use in your workflow any quantity computed on the test data, even for something as simple as data normalization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMP87jskSZuv"
      },
      "source": [
        "print (x_train_norm.max())\n",
        "print (x_train_norm.min())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amLdhSLI6gzT"
      },
      "source": [
        "A sample output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDL5Es5W6caw"
      },
      "source": [
        "print(x_train_norm[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYMsyOk8-xnf"
      },
      "source": [
        "## Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JBUukyQ0INrf"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezpCJySN5h2Y"
      },
      "source": [
        "x_train.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDlDs9YO5qk_"
      },
      "source": [
        "input_shape = x_train.shape[1]\n",
        "input_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZimwlA92lLr"
      },
      "source": [
        "The model can be created thi, for example with this layers:\n",
        "- [input] ==> [hidden] ==> [output]:\n",
        "  - 13 ==> [20] ==>  1\n",
        "\n",
        "The **Input Layer** should be 13 (number of features) and the **Output Layer** shoub be 1 to match the target (y). The number of neurons at **Hidden layers** are arbitrary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEdoqWl28dB3"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(input_shape),\n",
        "    tf.keras.layers.Dense(20, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNTU33Hn1TRj"
      },
      "source": [
        "Input layer has 13 conections, one for each feature [X]. Each feature goes to each one of the neurons of 1st Dense Layer, that has 20 Neurons. So, total parameters 1st Dense Layer will be ws=(13 x 20) + bs=20 ==> 280. The output layer will be only one Neuron that has one input from the output of previous layer (20 ) + 1 b ==> 21.\n",
        "\n",
        "For simplicity, the input layer can be \"merge with 1st layer\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXHoc5X42iR2"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(20, \n",
        "                          activation='relu', \n",
        "                          input_shape = [13]),\n",
        "    tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfGTI8SY_3AL"
      },
      "source": [
        "##Compile Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QzTQem0EHY-O"
      },
      "source": [
        "### Type of errors\n",
        "In statistics, `Mean Absolute Error (MAE)` is a measure of errors between paired observations expressing the same phenomenon. Examples of Y versus X include comparisons of predicted versus observed, subsequent time versus initial time, and one technique of measurement versus an alternative technique of measurement. MAE is calculated as:\n",
        "\n",
        "\n",
        "$$MAE=\\frac{1}{n}\\sum_{i=1}^{n}(\\left|y_{i}-\\hat{y}_{i}\\right|)$$\n",
        "\n",
        "\n",
        "Another alternative to evaluate regression is the `Root Mean Square Error (RMSE)`.\n",
        "This is the root of the  mean of the squared errors. It is a most popular measure of regression model's performance because also keep the same unit as y and larger errors are noted more than with MAE.\n",
        "\n",
        "$$RSME=\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n}{(y_{i}-\\hat{y}_{i})}^2}$$\n",
        "\n",
        "You can use MSE to calculate loss, but also tracking the MAE or RSME, once those values will have the \"same order\" of the Target (in the case, multiples of USD1,000)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7yHe1BU_Cr2"
      },
      "source": [
        "The optimizer used is [ADAM](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam), a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments. The hyperparameter \"Learning-Rate\" used is the default ==> 0.001"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA21O9Y0_BJ7"
      },
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse',\n",
        "    metrics=['mae'] # used to monitor the training and testing steps.\n",
        "    )"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJZ4fvT3_yd7"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RxklrX_R_uvn"
      },
      "source": [
        "history = model.fit(\n",
        "    x_train_norm, \n",
        "    y_train,\n",
        "    epochs=1000, \n",
        "    verbose=0\n",
        "    )"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xAXSo5-HCIo"
      },
      "source": [
        "Inspecting the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXzMCGzVUuTx"
      },
      "source": [
        "train_eval = model.evaluate(x_train_norm, y_train)\n",
        "print (\"Training data MSE: {:.2}\".format(train_eval[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShUfx2ZSCylX"
      },
      "source": [
        "history.history.keys()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npIubpXaKl6u"
      },
      "source": [
        "plt.plot(history.history['loss'], label='MSE')\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='upper right')\n",
        "#plt.ylim([0,50])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6qHpwg8BpZi"
      },
      "source": [
        "plt.plot(history.history['mae'], label='MAE')\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss in $K')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(loc='upper right')\n",
        "#plt.ylim([0,50])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYHTdEJVH0tZ"
      },
      "source": [
        "## Testing the trained model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QBuJGqbSH6xT"
      },
      "source": [
        "test_eval = model.evaluate(x_test_norm, y_test)\n",
        "print (\"Test data MAE: {:.2}\".format(test_eval[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4H-5TF42ZjL"
      },
      "source": [
        "rsme = round(np.sqrt(test_eval[0]), 3)\n",
        "rsme"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we1LXnkDzbaS"
      },
      "source": [
        "The model has an RSME error of around USD4,000 and an MAE of around USD 2,600, what is very good for house's price estimation.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_-PyEde1sAq"
      },
      "source": [
        "Note: With features **not normalized**, we got loss (MSE): 22.0815; RSME: USD4,700 and  MAE: USD3,500"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkaTLMaTIbIL"
      },
      "source": [
        "y_hat = model.predict(x_test_norm)\n",
        "print(y_hat[:5]) # get the output predict values for the 5 first samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p9buLk54A1W"
      },
      "source": [
        "y_test[:5] # get the output real known values for the 5 first samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERuEyCmx4QHY"
      },
      "source": [
        "plt.hist(y_hat, label='predictions', color = 'b')\n",
        "plt.hist(y_test, label = 'real values', color = 'r', alpha=0.5)\n",
        "plt.xlabel('Price in K$')\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQgmjypmxaFP"
      },
      "source": [
        "## Doing Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIior9FBvdcs"
      },
      "source": [
        "xt = np.array([1.1, 0., 9., 0., 0.6, 7., 92., 3.8 , 4., 300., 21., 200, 19.5])\n",
        "xt.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vp2mV37ZyOXE"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qp7KDtgyz8g"
      },
      "source": [
        "xt = np.reshape(xt, (1, 13))\n",
        "xt.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pLLiifrzO-3"
      },
      "source": [
        "xt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDlArmS2xfuR"
      },
      "source": [
        "xt_norm = scaler.transform(xt)\n",
        "xt_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHbXuOGkzJ-n"
      },
      "source": [
        "yt = model.predict(xt_norm)\n",
        "yt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfb5RGKT0rs3"
      },
      "source": [
        "xt = np.array([1.1, 0., 9., 0., 0.6, 7., 92., 3.8 , 4., 300., 21., 200, 19.5])\n",
        "xt = np.reshape(xt, (1, 13))\n",
        "xt_norm = scaler.transform(xt)\n",
        "yt = model.predict(xt_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUNbBC_FgcWZ"
      },
      "source": [
        "## Finding the correct Hyperparameters\n",
        "- [KerasTuner](https://keras.io/keras_tuner/)\n",
        "\n",
        "KerasTuner is an easy-to-use, scalable hyperparameter optimization framework that solves the pain points of hyperparameter search.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dql5K0r9gU1F"
      },
      "source": [
        "!pip install keras-tuner --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP1tw9u9g5Uf"
      },
      "source": [
        "import tensorflow as tf\n",
        "import keras_tuner as kt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLYVscNQkaWS"
      },
      "source": [
        "data = tf.keras.datasets.boston_housing\n",
        "(x_train, y_train), (x_test, y_test) = data.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pE7gSlcTkl20"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# first we fit the scaler on the training dataset\n",
        "scaler.fit(x_train)\n",
        "\n",
        "# then we call the transform method to scale both the training and testing data\n",
        "x_train_norm = scaler.transform(x_train)\n",
        "x_test_norm = scaler.transform(x_test)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UHfySq-hJHo"
      },
      "source": [
        "Write a function that creates and returns a Keras model. Use the `hp` argument to define the hyperparameters during model creation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbvjlmNdhBn6"
      },
      "source": [
        "def build_model(hp):\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      hp.Choice('units', [10, 20, 30]),\n",
        "      activation='relu'))\n",
        "  \n",
        "  model.add(tf.keras.layers.Dense(1))\n",
        "  model.compile(optimizer='adam', loss='mse')\n",
        "  return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsNpzZpeh2Hi"
      },
      "source": [
        "Initialize a tuner (here, RandomSearch). We use objective to specify the objective to select the best models, and we use max_trials to specify the number of different models to try."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zufG9sUjh1xV"
      },
      "source": [
        "tuner = kt.RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials=5)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN6Ci84OiFaZ"
      },
      "source": [
        "Start the search and get the best model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnkdoSzLiJUY"
      },
      "source": [
        "tuner.search(\n",
        "    x_train_norm, y_train, \n",
        "    epochs=500, \n",
        "    validation_data=(x_test_norm, y_test))\n",
        "\n",
        "best_model = tuner.get_best_models()[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70gOV5Idme4w"
      },
      "source": [
        "tuner.search_space_summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcF6Ogb_mtYW"
      },
      "source": [
        "tuner.results_summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}